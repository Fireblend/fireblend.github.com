---
layout: post
title: La TV tiene razón de tenerle miedo a la IA, pero por las razones equivocadas
---
{{page.title}}
================

Hace un par de semanas, Fox anunció su nuevo show de "ciencia ficción" "<b>nExt</b>". La premisa del show, otro de sus procedurales detectivescos, parece prometer una exploración de los peligros que nuestra pérdida de control sobre la tecnología supone, similar a Black Mirror o Phillip K. Dick's Electric Dreams:

<i>"neXt es un emocionante thriller basado en hechos acerca de la emergencia de una inteligencia artificial renegada y letal, que combina la acción con una examinación profunda de cómo la tecnología está invadiendo nuestras vidas y transformándolas de maneras que aún no entendemos"</i>

Sin embargo, <a href="https://www.youtube.com/watch?v=rRAoNVJcgms">el trailer no hace nada por convencer acerca de esa premisa</a>, protagonizando una copia a medias de Alexa, que cobrando voluntad propia, comienza a revelarse en contra de sus creadores y usuarios humanos con la sutileza y verbo de un villano de película de James Bond. Este enfoque estilo H.G. Wells de representar miedos sociales en la forma del "invasor externo" podrá haber servido para La Guerra de los Mundos o Godzilla, pero usado en el contexto de inteligencia artificial no solo supone ficción barata, sino que ayuda a encubrir las amenazas de las cuales deberíamos estar realmente preocupadxs acerca de algoritmos que actualmente toman decisiones que impactan vidas humanas de manera muy real.

<b>No (solo) es privacidad</b>

Es decir, lo es, pero no lo es todo. Debemos estar preocupados acerca de nuestros datos y hacia dónde van, pero cada vez se vuelve más difícil el evitar rendir una parte bastante significativa de nuestros gustos, comportamientos y ser al mismo tiempo que se actúa como parte de la sociedad, y muchas empresas se dedican a exprimir tanto valor como sea posible de ellos, siendo poco lo que podemos lograr sin convertirnos en ermitaños digitales.

Es difícil no hablar de inevitabilidad. No solo es posible el ser identificado y seguido por medio de clicks y términos de búsquedas, sino que también por nuestro comportamiento físico al interactuar con sistemas de computación. Aquellos que tapan la webcam de su laptop pueden tener la idea correcta, pero tal vez no hayan escuchado de biometría conductual, técnicas por medio de las cuales personas pueden ser identificadas por el ritmo con el que escriben en su teclado, o los patrones de movimiento del mouse sobre la pantalla (cómo parar eso <a href="https://www.tripwire.com/state-of-security/security-awareness/how-to-bust-keyboard-biometrics-and-why-you-might-want-to/">acá</a>).

Esto no quiere decir que no debamos intentar minimizar nuestra huella de datos, pero sí que los esfuerzos de privacidad y protección de datos también deben enfocarse en la exigencia por parte del público a favor de políticas estrictas de seguridad y confidencialidad. <a href="https://www.vox.com/recode/2019/5/27/18639284/duckduckgo-gabe-weinberg-do-not-track-privacy-legislation-kara-swisher-decode-podcast-interview">En un episodio reciente del podcast Recode Decode de Vox</a>, <b>Gabe Weinberg</b>, CEO de DuckDuckGo correctamente asevera que el uso de datos de usuarios debe ser algo a lo que ellos deban expresar consentimiento explícito, no implementar procesos engañosos o molestos para renunciar a dicha participación, un punto de vista con el que notablemente varios países en Europa están de acuerdo, pero que aún debe expandirse al resto del mundo para ser efectiva.

<b>Tácticas de cambio conductual</b>

Vivimos en un mundo capitalista, y las empresas hambrientas de dinero han empezado a darse cuenta de que no sólo puede usar sus recursos y herramientas para recolectar datos y predecir comportamientos emergentes, sino que pueden accionar dichos descubrimientos hacia la modificación de nuestro comportamiento, efectivamente apostando sobre nuestro futuro y arreglando dichas apuestas. En el libro <b>The Age of Surveillance Capitalism</b>, de <b>Shoshana Zuboff</b>, la autora <a href="https://www.theguardian.com/technology/2019/jan/20/shoshana-zuboff-age-of-surveillance-capitalism-google-facebook">argumenta que el objetivo para los behavioral analytics bajo el lente del capitalismo puede representar una amenaza a nuestra privacidad y libre albedrío</a>, un panorama determinista de decisiones tomadas y profecías auto-cumplidoras:

<i>"El capitalismo de la vigilancia declara de manera unilateral la experiencia humana como materia prima para su traducción a datos conductuales. A pesar de que algunos de estos datos se apliquen a mejora de servicios, el resto se declaran como restos conductuales propietarios, alimentando procesos de manufactura avanzados conocidos como "inteligencia de máquina", y fabricados en productos de producción que anticipan lo que hacemos ahora, pronto y luego. Finalmente, estos productos de predicción se intercambian en un nuevo tipo de mercados a los que llamo mercados futuros conductuales. Los capitalistas de la vigilancia se han vuelto inmensamente valiosos a partir de dichas operaciones, pues muchas compañías están dispuestas a apostar sobre nuestro comportamiento futuro"</i>

Las implicaciones del data tracking no regulado y la manipulación de estos datos bajo un paradigma capitalista deben ser evidentes. Desde amenazas a nuestros comportamientos hasta a la democracia misma, no necesitamos un super-villano computacional para imaginar escenarios donde el uso indebido de la IA acabe en catástrofe. Un enfoque fuertemente basado en principios éticos para la elaboración de políticas de datos es fundamental si queremos sobrevivir esta nueva era digital con nuestra autonomía intacta.

<b>No hay tecnología "Libre de Sesgos"</b>

Los sesgos cognitivos se encuentran por todas partes, y los algoritmos no nos están ayudando, sino exacerbando y ocultando el problema. <b>Los datos y los algoritmos no son un reflejo fiel u objetivo de nuestra sociedad; llevan con ellos exáctamente los mismos prejuicios, sesgos y malentendidos que las culturas y organizaciones en las cuales son creados</b>, excepto que esta vez se esconden dentro de paquetes de software que obstaculizan la atribución de errores y son fáciles de justificar al confundir determinismo por objetividad. Cuando implementamos sistemas que deciden quién recibe atención médica, a quién sospechar de algún crimen o qué contenido recomendar a usuarios jóvenes, es algo acerca de qué preocuparse.

<b>Rachel Thomas</b>, profesora en la <b>Universidad del Instituto de Datos de San Francisco</b> y cofundadora de <b>fast.ai</b>, ha <a href="https://www.fast.ai/2019/01/29/five-scary-things/">hablado y escrito acerca de este tema a profundidad</a>:

<i>"nuestros algoritmos y productos impactan la sociedad y son parte de ciclos de feedback. Considere un algoritmo que prediga el crimen y determine hacia dónde enviar oficiales de policia: enviar mas oficiales a un vecindario en particular no sólo es un efecto, sino también una causa. Más policías pueden llevar a más arrestos en dicho vecindario, lo cual podría llevar al algoritmo a enviar aún más policías (un mecanismo descrito en este artículo acerca de ciclos de feedback sin control)"</i>

Algoritmos de visión de computadora tienen <a href="https://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html">peores resultados en personas de razas no-caucásicas</a> porque la mayoría de datos faciales disponibles provienen de personas de descendencia Europea. La mayor cantidad de datos de comportamiento en línea <a href="https://arxiv.org/abs/1711.08536">proviene de regiones socialmente altas en países desarrollados</a>. Los algoritmos de auto-completado tienden a predecir palabras asociadas con carreras de ingeniería y matemática al ser juntadas con nombres masculinos mientras que recomiendan términos de labor doméstico para nombre femeninos. Estos errores no provienen de la malicia personal de quienes implementan dichos algoritmos, sino que reflejan <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">los sesgos cognitivos implícitos en la sociedad que provee los datos para su funcionamiento</a>, y deben ser un factor de consideración.

Lejos de ser perfectos árbitros de la verdad, los algoritmos y los datos llevan consigo nuestras debilidades cognitivas, y la falta de entendimiento de este hecho por parte de aquellos que implementan, mantienen, consumen y regulan estos servicios puede llevar a que caminemos ciegos hacia el precipicio, de la mano de máquinas prejuiciosas y fácilmente manipulables de nuestra propia creación. Consideraciones éticas deben ser parte de sistemas de IA desde su concepción; no podemos esperar hasta que el daño esté hecho o confiar en que la amenaza de enormes sanciones financieras van a ser suficientes.

<center><a href="https://i.imgur.com/h7np6Gw.png"><img src="https://i.imgur.com/h7np6Gw.png" width="600px"/></a></center>

<b>¿Qué hacer al respecto?</b>

Desafortunadamente, solucionar estos problemas no es cuestión de llamar a un héroe de acción para traer abajo a la prima malvada de Alexa, tal como neXt nos lo haría creer. No hay un "otro" en este escenario más que la potenciación indiscriminada de nuestros prejuicios y malentendidos, empacados en una caja atractiva de determinismo, mejores cortinas para esconder nuestros sesgos cognitivos.

Sin embargo, hemos sido capaces de lidiar con sesgos en el pasado; los humanos somos lo suficientemente conscientes de su existencia como para identificarlos y corregirlos (con varios grados de éxito). El desafío está en aplicar estas soluciones a sistemas que podríamos haber imaginado en el futuro serían justos u objetivos, y hacer que las personas que actuan sobre cada paso de estos procesos de datos: su recolección, mantenimiento, exploración, modelado e implementación sean conscientes de dichas limitaciones y responsables de las mismas. Es también necesario reconocer que sesgos como estos son difíciles de identificar, y considerar la posibilidad de que nuestros algoritmos pueden cometer errores, de manera más frecuente e impactante de la prevista, haciendo disponibles maneras de corregirlos o incluso eliminarlos de manera proactiva y no cuando el daño esté hecho.

<a href="https://www.infoq.com/presentations/unconscious-bias-machine-learning">En una presentación de <b>InfoQ</b></a>, <b>Rachel Thomas</b> esboza varias soluciones a problemas descritos acá desde el punto de vista de empresas de datos, tales como:

- <b>No maximizar métricas</b>: Es fácil caer en la tentación de querer ver números que suben, pero mientras más se reducen los datos a cifras, es más fácil obviar cómo se puede estar recompensando comportamientos negativos en los servicios que se ofrecen o habilitando ciclos de feedback que restringen dichos servicios injustamente.

- <b>Reclutar equipos diversos</b>: Estudios muestran que los equipos diversos tienen mejor desempeño, y cuando de análisis de datos se trata, pueden contribuir a un mejor entendimiento del dominio del problema de los datasets con los que se trabaja, llevando a descubrimientos accionables significativos y la minimización de sesgos introducidos a nivel interno.

- <b>Pedir políticas adecuadas</b>: La autora argumenta que "La tecnología avanzada no es un substituto de buenas políticas". Los sesgos pueden ser difíciles de identificar, sin importar que tan bueno el proceso o los algoritmos, deben existir regulaciones y protocolos que garanticen la disponibilidad de procesos de apelación a decisiones puestas en marcha por sistemas automáticos, sus resultados validados y daño prevenido o deshecho a un nivel mayor que las propias empresas de datos.

- <b>No detenerse</b>: Nunca se llegará a un punto donde seamos libres de nuestros algoritmos o sesgos, así es como acabamos en esta situación en primer lugar. Es necesario entrenar a personas para que siempre estén buscando potenciales ciclos de feedback, sesgos y prejuicios implícitos en nuestros datos. Dejemos los escenarios de objetividad definitiva en el mundo de la ciencia ficción barata.
